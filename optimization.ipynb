{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amr552/Deep-Learning/blob/NLP/optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Genetic Algorithm"
      ],
      "metadata": {
        "id": "olRCG1JN5CLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load your image dataset (replace with your actual data loading code)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_dataset = '/content/drive/MyDrive/datasets/plant_disease/Train/Train'\n",
        "val_dataset = '/content/drive/MyDrive/datasets/plant_disease/Validation/Validation'\n",
        "test_dataset = '/content/drive/MyDrive/datasets/plant_disease/Test/Test'\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dataset,\n",
        "    target_size=(28, 28),  # Adjust image size as needed\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    val_dataset,\n",
        "    target_size=(28, 28),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dataset,\n",
        "    target_size=(28, 28),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "def create_cnn_model(params):\n",
        "    learning_rate, num_filters, kernel_size, stride, pooling_type, pooling_size, batch_size, optimizer_name = params\n",
        "\n",
        "    if pooling_type == 'max':\n",
        "        PoolingLayer = MaxPooling2D\n",
        "    else:\n",
        "        PoolingLayer = AveragePooling2D\n",
        "\n",
        "    optimizers = {\n",
        "        'adam': tf.keras.optimizers.Adam(learning_rate),\n",
        "        'sgd': tf.keras.optimizers.SGD(learning_rate),\n",
        "        'rmsprop': tf.keras.optimizers.RMSprop(learning_rate)\n",
        "    }\n",
        "\n",
        "    optimizer = optimizers[optimizer_name]\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(num_filters, (kernel_size, kernel_size), strides=stride, activation='relu', input_shape=(28, 28, 3)))\n",
        "    model.add(Conv2D(num_filters, (kernel_size, kernel_size), strides=stride, activation='relu',padding='same'))\n",
        "    model.add(PoolingLayer(pool_size=(pooling_size, pooling_size)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))  # Output layer for 3 classes\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def fitness(params):\n",
        "    learning_rate, num_filters, kernel_size, stride, pooling_type, pooling_size, batch_size, optimizer_name = params\n",
        "    model = create_cnn_model(params)\n",
        "\n",
        "    # Fit model for a few epochs to evaluate\n",
        "    model.fit(train_generator, epochs=1, batch_size=batch_size, verbose=0)  # Adjust training epochs as needed\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    loss, accuracy = model.evaluate(validation_generator, verbose=0)\n",
        "    return accuracy\n",
        "\n",
        "def genetic_algorithm():\n",
        "    # Define hyperparameter ranges (adjust these as needed)\n",
        "    learning_rate_range = (0.001, 0.1)\n",
        "    num_filters_range = (16, 64)\n",
        "    kernel_size_range = (3, 5)\n",
        "    stride_range = (1, 2)\n",
        "    pooling_type_options = ['max', 'avg']\n",
        "    pooling_size_range = (2, 3)\n",
        "    batch_size_range = (16, 64)\n",
        "    optimizer_names = ['adam', 'sgd', 'rmsprop']\n",
        "\n",
        "    # Generate initial population of solutions\n",
        "    solutions = []\n",
        "    for _ in range(100):\n",
        "        learning_rate = random.uniform(*learning_rate_range)\n",
        "        num_filters = random.randint(*num_filters_range)\n",
        "        kernel_size = random.randint(*kernel_size_range)\n",
        "        stride = random.randint(*stride_range)\n",
        "        pooling_type = random.choice(pooling_type_options)\n",
        "        pooling_size = random.randint(*pooling_size_range)\n",
        "        batch_size = random.randint(*batch_size_range)\n",
        "        optimizer_name = random.choice(optimizer_names)\n",
        "        solutions.append((learning_rate, num_filters, kernel_size, stride, pooling_type, pooling_size, batch_size, optimizer_name))\n",
        "\n",
        "    for generation in range(100):\n",
        "        ranked_solutions = []\n",
        "        for solution in solutions:\n",
        "            fitness_value = fitness(solution)\n",
        "            ranked_solutions.append((fitness_value, solution))\n",
        "\n",
        "        ranked_solutions.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "\n",
        "        print(f'=== Gen {generation} best solutions ===')\n",
        "        print(ranked_solutions[0])\n",
        "\n",
        "        if ranked_solutions[0][0] > 0.99:\n",
        "            break\n",
        "\n",
        "        # Selection (choose best solutions for breeding)\n",
        "        best_solutions = ranked_solutions[:20]\n",
        "\n",
        "        # Crossover (combine genes from parents to create offspring)\n",
        "        new_generation = []\n",
        "        for _ in range(100):\n",
        "            parent1 = random.choice(best_solutions)[1]\n",
        "            parent2 = random.choice(best_solutions)[1]\n",
        "            offspring = (\n",
        "                random.choice([parent1[0], parent2[0]]),  # Learning rate\n",
        "                random.choice([parent1[1], parent2[1]]),  # Number of filters\n",
        "                random.choice([parent1[2], parent2[2]]),  # Kernel size\n",
        "                random.choice([parent1[3], parent2[3]]),  # Stride\n",
        "                random.choice([parent1[4], parent2[4]]),  # Pooling type\n",
        "                random.choice([parent1[5], parent2[5]]),  # Pooling size\n",
        "                random.choice([parent1[6], parent2[6]]),  # Batch size\n",
        "                random.choice([parent1[7], parent2[7]])   # Optimizer name\n",
        "            )\n",
        "            new_generation.append(offspring)\n",
        "\n",
        "        # Mutation (randomly alter some offspring)\n",
        "        for i in range(len(new_generation)):\n",
        "            if random.random() < 0.1:  # 10% mutation rate\n",
        "                new_generation[i] = (\n",
        "                    random.uniform(*learning_rate_range),\n",
        "                    random.randint(*num_filters_range),\n",
        "                    random.randint(*kernel_size_range),\n",
        "                    random.randint(*stride_range),\n",
        "                    random.choice(pooling_type_options),\n",
        "                    random.randint(*pooling_size_range),\n",
        "                    random.randint(*batch_size_range),\n",
        "                    random.choice(optimizer_names)\n",
        "                )\n",
        "\n",
        "        solutions = new_generation\n",
        "\n",
        "    # Return the best solution\n",
        "    best_solution = ranked_solutions[0]\n",
        "    print(f'Best solution: {best_solution[1]} with accuracy: {best_solution[0]}')\n",
        "\n",
        "# Run the genetic algorithm\n",
        "genetic_algorithm()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl10C0Jxa1vy",
        "outputId": "7bb9fce6-1acc-4bb0-9217-b00daadf98dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1322 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 150 images belonging to 3 classes.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7be447914c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7be447914af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Gen 0 best solutions ===\n",
            "(0.6000000238418579, (0.05162399086507868, 64, 5, 1, 'max', 3, 38, 'sgd'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYkALp73SvQa",
        "outputId": "de0a3afc-6608-4608-ab55-b6b2041b797e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1322 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Found 150 images belonging to 3 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7f548aa4ea70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f548aa4c4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===Gen 0 best solutions ===\n",
            "(0.6666666865348816, (0.004003320733649794, 31, 3))\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load your image dataset (replace with your actual data loading code)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_dataset='/content/drive/MyDrive/datasets/plant_disease/Train/Train'\n",
        "val_dataset='/content/drive/MyDrive/datasets/plant_disease/Validation/Validation'\n",
        "test_dataset='/content/drive/MyDrive/datasets/plant_disease/Test/Test'\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dataset,\n",
        "    target_size=(28, 28),  # Adjust image size as needed\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    val_dataset,\n",
        "    target_size=(28, 28),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dataset,\n",
        "    target_size=(28, 28),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "def create_cnn_model(learning_rate, num_filters, kernel_size):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(num_filters, (kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 3)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(num_filters * 2, (kernel_size, kernel_size), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))  # Output layer for 3 classes\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def fitness(params):\n",
        "    learning_rate, num_filters, kernel_size = params\n",
        "    model = create_cnn_model(learning_rate, num_filters, kernel_size)\n",
        "\n",
        "    # Fit model for one epoch to evaluate\n",
        "    model.fit(train_generator, epochs=5, verbose=0)  # Adjust training epochs as needed\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    loss, accuracy = model.evaluate(validation_generator, verbose=0)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def genetic_algorithm():\n",
        "    # Define hyperparameter ranges (adjust these as needed)\n",
        "    learning_rate_range = (0.001, 0.1)\n",
        "    num_filters_range = (16, 64)\n",
        "    kernel_size_range = (3, 5)\n",
        "\n",
        "    # Generate initial population of solutions\n",
        "    solutions = []\n",
        "    for _ in range(100):\n",
        "        learning_rate = random.uniform(*learning_rate_range)\n",
        "        num_filters = random.randint(*num_filters_range)\n",
        "        kernel_size = random.randint(*kernel_size_range)\n",
        "        solutions.append((learning_rate, num_filters, kernel_size))\n",
        "\n",
        "    for generation in range(100):\n",
        "        ranked_solutions = []\n",
        "        for solution in solutions:\n",
        "            fitness_value = fitness(solution)\n",
        "            ranked_solutions.append((fitness_value, solution))\n",
        "\n",
        "        ranked_solutions.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        print(f'===Gen {generation} best solutions ===')\n",
        "        print(ranked_solutions[0])\n",
        "\n",
        "        if ranked_solutions[0][0] > 0.99:\n",
        "            break\n",
        "\n",
        "        # Selection (choose best solutions for breeding)\n",
        "        best_solutions = ranked_solutions[:20]\n",
        "\n",
        "        # Crossover (combine genes from parents to create offspring)\n",
        "        new_generation = []\n",
        "        for _ in range(100):\n",
        "            parent1 = random.choice(best_solutions)[1]\n",
        "            parent2 = random.choice(best_solutions)[1]\n",
        "            offspring = (\n",
        "                random.choice([parent1[0], parent2[0]]),  # Learning rate\n",
        "                random.choice([parent1[1], parent2[1]]),  # Number of filters\n",
        "                random.choice([parent1[2], parent2[2]])   # Kernel size\n",
        "            )\n",
        "            new_generation.append(offspring)\n",
        "\n",
        "        # Mutation (randomly alter some offspring)\n",
        "        for i in range(len(new_generation)):\n",
        "            if random.random() < 0.1:  # 10% mutation rate\n",
        "                new_generation[i] = (\n",
        "                    random.uniform(*learning_rate_range),\n",
        "                    random.randint(*num_filters_range),\n",
        "                    random.randint(*kernel_size_range)\n",
        "                )\n",
        "\n",
        "        solutions = new_generation\n",
        "\n",
        "    # Return the best solution\n",
        "    best_solution = ranked_solutions[0]\n",
        "    print(f'Best solution: {best_solution[1]} with accuracy: {best_solution[0]}')\n",
        "\n",
        "# Run the genetic algorithm\n",
        "genetic_algorithm()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PSO Algorithm for getting the ideal parameters of a simple CNN model"
      ],
      "metadata": {
        "id": "gxaC3T_K62xQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2aSZVr-GuNn",
        "outputId": "24f15140-7f7a-4d8b-a1e9-309bc8539458"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1322 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Iteration 0, Best accuracy: 0.5333\n",
            "Iteration 1, Best accuracy: 0.6500\n",
            "Iteration 2, Best accuracy: 0.6500\n",
            "Iteration 3, Best accuracy: 0.6500\n",
            "Iteration 4, Best accuracy: 0.6500\n",
            "Iteration 5, Best accuracy: 0.6500\n",
            "Iteration 6, Best accuracy: 0.6500\n",
            "Iteration 7, Best accuracy: 0.6500\n",
            "Iteration 8, Best accuracy: 0.6500\n",
            "Iteration 9, Best accuracy: 0.6667\n",
            "Best solution: [0.03888646513457217, 64, 3, 'max', 'rmsprop'] with accuracy: 0.6667\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "\n",
        "# Load your image dataset (replace with your actual data loading code)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_dataset = '/content/drive/MyDrive/datasets/plant_disease/Train/Train'\n",
        "val_dataset = '/content/drive/MyDrive/datasets/plant_disease/Validation/Validation'\n",
        "test_dataset = '/content/drive/MyDrive/datasets/plant_disease/Test/Test'\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dataset,\n",
        "    target_size=(28, 28),  # Adjust image size as needed\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    val_dataset,\n",
        "    target_size=(28, 28),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "def create_cnn_model(learning_rate, num_filters, kernel_size, pooling_type, optimizer_name):\n",
        "    model = Sequential()\n",
        "    kernel_size = int(kernel_size)\n",
        "    num_filters = int(num_filters)\n",
        "\n",
        "    model.add(Conv2D(num_filters, (kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 3)))\n",
        "\n",
        "    if pooling_type == 'max':\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "    elif pooling_type == 'avg':\n",
        "        model.add(AveragePooling2D((2, 2)))\n",
        "    else:\n",
        "        raise ValueError(\"Invalid pooling type\")\n",
        "\n",
        "    model.add(Conv2D(num_filters * 2, (kernel_size, kernel_size), activation='relu'))\n",
        "    model.add(MaxPooling2D((3, 3)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))  # Output layer for 3 classes\n",
        "\n",
        "    if optimizer_name == 'adam':\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer_name == 'rmsprop':\n",
        "        optimizer = RMSprop(learning_rate=learning_rate)\n",
        "    elif optimizer_name == 'sgd':\n",
        "        optimizer = SGD(learning_rate=learning_rate)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid optimizer name\")\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def fitness(params):\n",
        "    learning_rate, num_filters, kernel_size, pooling_type, optimizer_name = params\n",
        "    model = create_cnn_model(learning_rate, num_filters, kernel_size, pooling_type, optimizer_name)\n",
        "\n",
        "    # Fit the model for one epoch (adjust as needed)\n",
        "    model.fit(train_generator, epochs=1, verbose=0)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    loss, accuracy = model.evaluate(validation_generator, verbose=0)\n",
        "    return accuracy  # Only return the accuracy metric\n",
        "\n",
        "def particle_swarm_optimization():\n",
        "    # Define hyperparameter ranges (adjust these as needed)\n",
        "    learning_rate_range = (0.001, 0.1)\n",
        "    num_filters_range = (16, 64)\n",
        "    kernel_size_range = (3, 5)\n",
        "    pooling_type_range = ('max', 'avg')\n",
        "    optimizer_range = ('adam', 'rmsprop', 'sgd')\n",
        "\n",
        "    # Initialize particles\n",
        "    num_particles = 10  # Reduced number for testing\n",
        "    particles = []\n",
        "    velocities = []\n",
        "    for _ in range(num_particles):\n",
        "        particle = [\n",
        "            random.uniform(*learning_rate_range),\n",
        "            random.randint(*num_filters_range),\n",
        "            random.randint(*kernel_size_range),\n",
        "            random.choice(pooling_type_range),\n",
        "            random.choice(optimizer_range)\n",
        "        ]\n",
        "        velocity = [\n",
        "            random.uniform(-1, 1),\n",
        "            random.uniform(-1, 1),\n",
        "            random.uniform(-1, 1),\n",
        "            random.uniform(-1, 1),\n",
        "            random.uniform(-1, 1)\n",
        "        ]\n",
        "        particles.append(particle)\n",
        "        velocities.append(velocity)\n",
        "\n",
        "    # Personal best and global best\n",
        "    pbest = particles.copy()\n",
        "    pbest_fitness = [fitness(p) for p in particles]\n",
        "    gbest = particles[np.argmax(pbest_fitness)]\n",
        "    gbest_fitness = max(pbest_fitness)\n",
        "\n",
        "    # Particle Swarm Optimization\n",
        "    for iteration in range(10):  # Reduced iterations for testing\n",
        "        for i in range(num_particles):\n",
        "            # Update velocity\n",
        "            w = 0.9\n",
        "            c1 = 2\n",
        "            c2 = 2\n",
        "            r1 = random.random()\n",
        "            r2 = random.random()\n",
        "            velocities[i] = [\n",
        "                w * velocities[i][0] + c1 * r1 * (pbest[i][0] - particles[i][0]) + c2 * r2 * (gbest[0] - particles[i][0]),\n",
        "                w * velocities[i][1] + c1 * r1 * (pbest[i][1] - particles[i][1]) + c2 * r2 * (gbest[1] - particles[i][1]),\n",
        "                w * velocities[i][2] + c1 * r1 * (pbest[i][2] - particles[i][2]) + c2 * r2 * (gbest[2] - particles[i][2]),\n",
        "                velocities[i][3],  # No change for categorical params\n",
        "                velocities[i][4]   # No change for categorical params\n",
        "            ]\n",
        "\n",
        "            # Update particle position\n",
        "            particles[i] = [\n",
        "                particles[i][0] + velocities[i][0],\n",
        "                int(particles[i][1] + velocities[i][1]),  # Ensure integer values\n",
        "                int(particles[i][2] + velocities[i][2]),  # Ensure integer values\n",
        "                particles[i][3],\n",
        "                particles[i][4]\n",
        "            ]\n",
        "\n",
        "            # Ensure the hyperparameters are within valid ranges\n",
        "            particles[i][0] = max(min(particles[i][0], learning_rate_range[1]), learning_rate_range[0])\n",
        "            particles[i][1] = max(min(particles[i][1], num_filters_range[1]), num_filters_range[0])\n",
        "            particles[i][2] = max(min(particles[i][2], kernel_size_range[1]), kernel_size_range[0])\n",
        "\n",
        "            # Evaluate particle fitness\n",
        "            particle_fitness = fitness(particles[i])\n",
        "\n",
        "            # Update personal best\n",
        "            if particle_fitness > pbest_fitness[i]:\n",
        "                pbest[i] = particles[i]\n",
        "                pbest_fitness[i] = particle_fitness\n",
        "\n",
        "            # Update global best\n",
        "            if particle_fitness > gbest_fitness:\n",
        "                gbest = particles[i]\n",
        "                gbest_fitness = particle_fitness\n",
        "\n",
        "        print(f'Iteration {iteration}, Best accuracy: {gbest_fitness:.4f}')\n",
        "\n",
        "        if gbest_fitness > 0.99:  # Arbitrary stopping condition for testing\n",
        "            break\n",
        "\n",
        "    print(f'Best solution: {gbest} with accuracy: {gbest_fitness:.4f}')\n",
        "\n",
        "# Run the Particle Swarm Optimization\n",
        "particle_swarm_optimization()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTtq0I3iFp7I",
        "outputId": "088785cf-e32b-462f-e73c-620e3fc06a2e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1322 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x780e8d5f89d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x780e8da064d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Best accuracy: 0.5833\n",
            "Accuracy 0.5833 of parameters [[0.1, 29, 5, 'avg', 'sgd'], [0.1, 28, 3, 'avg', 'adam'], [0.001, 29, 3, 'avg', 'adam'], [0.001, 30, 4, 'max', 'rmsprop'], [0.1, 61, 5, 'max', 'rmsprop'], [0.1, 19, 3, 'avg', 'rmsprop'], [0.1, 28, 3, 'max', 'rmsprop'], [0.1, 32, 4, 'max', 'adam'], [0.1, 21, 3, 'avg', 'rmsprop'], [0.001, 32, 3, 'max', 'rmsprop']]\n",
            "Iteration 1, Best accuracy: 0.5833\n",
            "Accuracy 0.5833 of parameters [[0.1, 31, 5, 'avg', 'sgd'], [0.09356995056332348, 28, 3, 'avg', 'adam'], [0.001, 16, 3, 'avg', 'adam'], [0.001, 22, 3, 'max', 'rmsprop'], [0.1, 16, 3, 'max', 'rmsprop'], [0.1, 64, 5, 'avg', 'rmsprop'], [0.08965386785905946, 24, 4, 'max', 'rmsprop'], [0.09148799288351903, 42, 4, 'max', 'adam'], [0.1, 46, 5, 'avg', 'rmsprop'], [0.001, 16, 3, 'max', 'rmsprop']]\n",
            "Iteration 2, Best accuracy: 0.5833\n",
            "Accuracy 0.5833 of parameters [[0.1, 27, 4, 'avg', 'sgd'], [0.03994330304062848, 29, 4, 'avg', 'adam'], [0.001, 16, 3, 'avg', 'adam'], [0.001, 34, 4, 'max', 'rmsprop'], [0.0834292353215084, 16, 3, 'max', 'rmsprop'], [0.1, 56, 5, 'avg', 'rmsprop'], [0.04783699452262091, 24, 5, 'max', 'rmsprop'], [0.0036981065101913124, 56, 4, 'max', 'adam'], [0.1, 46, 5, 'avg', 'rmsprop'], [0.001, 16, 4, 'max', 'rmsprop']]\n",
            "Iteration 3, Best accuracy: 0.5833\n",
            "Accuracy 0.5833 of parameters [[0.1, 26, 4, 'avg', 'sgd'], [0.003267781577770759, 30, 5, 'avg', 'adam'], [0.001, 16, 3, 'avg', 'adam'], [0.001, 32, 5, 'max', 'rmsprop'], [0.007297525632894819, 16, 5, 'max', 'rmsprop'], [0.1, 39, 5, 'avg', 'rmsprop'], [0.006742546380935556, 26, 5, 'max', 'rmsprop'], [0.001, 36, 4, 'max', 'adam'], [0.1, 16, 4, 'avg', 'rmsprop'], [0.001, 19, 3, 'max', 'rmsprop']]\n",
            "Iteration 4, Best accuracy: 0.5833\n",
            "Accuracy 0.5833 of parameters [[0.1, 36, 5, 'avg', 'sgd'], [0.1, 27, 3, 'avg', 'adam'], [0.001, 16, 3, 'avg', 'adam'], [0.001, 26, 4, 'max', 'rmsprop'], [0.054612255853410466, 64, 5, 'max', 'rmsprop'], [0.1, 50, 5, 'avg', 'rmsprop'], [0.009680578986584927, 30, 5, 'max', 'rmsprop'], [0.038582525654671296, 16, 4, 'max', 'adam'], [0.1, 20, 4, 'avg', 'rmsprop'], [0.001, 22, 3, 'max', 'rmsprop']]\n",
            "Iteration 5, Best accuracy: 0.5833\n",
            "Accuracy 0.5833 of parameters [[0.08506819359429468, 33, 5, 'avg', 'sgd'], [0.036624925334415595, 31, 4, 'avg', 'adam'], [0.001, 27, 4, 'avg', 'adam'], [0.001, 30, 3, 'max', 'rmsprop'], [0.09302077036023537, 64, 5, 'max', 'rmsprop'], [0.1, 55, 4, 'avg', 'rmsprop'], [0.05288888457353374, 27, 4, 'max', 'rmsprop'], [0.09009897075244186, 16, 4, 'max', 'adam'], [0.055763054287756415, 58, 5, 'avg', 'rmsprop'], [0.001, 30, 3, 'max', 'rmsprop']]\n",
            "Iteration 6, Best accuracy: 0.5833\n",
            "Accuracy 0.5833 of parameters [[0.07607948896807232, 33, 4, 'avg', 'sgd'], [0.001, 33, 5, 'avg', 'adam'], [0.001, 35, 4, 'avg', 'adam'], [0.001, 33, 3, 'max', 'rmsprop'], [0.07533079058273759, 64, 5, 'max', 'rmsprop'], [0.048468660902162154, 48, 5, 'avg', 'rmsprop'], [0.09154583307266675, 25, 3, 'max', 'rmsprop'], [0.001, 51, 4, 'max', 'adam'], [0.001, 47, 5, 'avg', 'rmsprop'], [0.001, 36, 4, 'max', 'rmsprop']]\n",
            "Iteration 7, Best accuracy: 0.5833\n",
            "Accuracy 0.5833 of parameters [[0.04453849509893808, 30, 3, 'avg', 'sgd'], [0.05616741327774325, 27, 4, 'avg', 'adam'], [0.051567871969740615, 22, 4, 'avg', 'adam'], [0.001, 31, 4, 'max', 'rmsprop'], [0.03282014442403901, 64, 5, 'max', 'rmsprop'], [0.012959522015685468, 32, 5, 'avg', 'rmsprop'], [0.00259537655560646, 32, 5, 'max', 'rmsprop'], [0.001, 41, 4, 'max', 'adam'], [0.001, 24, 4, 'avg', 'rmsprop'], [0.001, 29, 5, 'max', 'rmsprop']]\n",
            "Iteration 8, Best accuracy: 0.5833\n",
            "Accuracy 0.5833 of parameters [[0.042091806580881845, 29, 3, 'avg', 'sgd'], [0.09352582705125215, 24, 3, 'avg', 'adam'], [0.001, 16, 3, 'avg', 'adam'], [0.02060026087533988, 28, 4, 'max', 'rmsprop'], [0.03416409989135097, 64, 4, 'max', 'rmsprop'], [0.006860859319095425, 28, 5, 'avg', 'rmsprop'], [0.001, 35, 5, 'max', 'rmsprop'], [0.001, 16, 4, 'max', 'adam'], [0.07521836147216807, 26, 4, 'avg', 'rmsprop'], [0.001, 24, 3, 'max', 'rmsprop']]\n",
            "Iteration 9, Best accuracy: 0.5833\n",
            "Accuracy 0.5833 of parameters [[0.08921793904317565, 34, 5, 'avg', 'sgd'], [0.001, 41, 5, 'avg', 'adam'], [0.0023057736800076714, 20, 3, 'avg', 'adam'], [0.06651146217198772, 33, 3, 'max', 'rmsprop'], [0.08944172261999005, 34, 5, 'max', 'rmsprop'], [0.1, 64, 3, 'avg', 'rmsprop'], [0.001, 34, 5, 'max', 'rmsprop'], [0.03261196127188791, 28, 4, 'max', 'adam'], [0.08987998135240481, 40, 4, 'avg', 'rmsprop'], [0.001, 26, 3, 'max', 'rmsprop']]\n",
            "Best solution: [0.04880214888829514, 29, 4, 'avg', 'adam'] with accuracy: 0.5833\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "\n",
        "# Load your image dataset (replace with your actual data loading code)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_dataset = '/content/drive/MyDrive/datasets/plant_disease/Train/Train'\n",
        "val_dataset = '/content/drive/MyDrive/datasets/plant_disease/Validation/Validation'\n",
        "test_dataset = '/content/drive/MyDrive/datasets/plant_disease/Test/Test'\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dataset,\n",
        "    target_size=(28, 28),  # Adjust image size as needed\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    val_dataset,\n",
        "    target_size=(28, 28),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Define the CNN model creation function\n",
        "def create_cnn_model(learning_rate, num_filters, kernel_size, pooling_type, optimizer_name):\n",
        "    model = Sequential()\n",
        "    kernel_size = int(kernel_size)\n",
        "    num_filters = int(num_filters)\n",
        "\n",
        "    model.add(Conv2D(num_filters, (kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 3)))\n",
        "\n",
        "    if pooling_type == 'max':\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "    elif pooling_type == 'avg':\n",
        "        model.add(AveragePooling2D((2, 2)))\n",
        "    else:\n",
        "        raise ValueError(\"Invalid pooling type\")\n",
        "\n",
        "    model.add(Conv2D(num_filters * 2, (kernel_size, kernel_size), activation='relu'))\n",
        "    model.add(MaxPooling2D((3, 3)))\n",
        "    model.add(Conv2D(num_filters * 2, (kernel_size, kernel_size), padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))  # Output layer for 3 classes\n",
        "\n",
        "    if optimizer_name == 'adam':\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer_name == 'rmsprop':\n",
        "        optimizer = RMSprop(learning_rate=learning_rate)\n",
        "    elif optimizer_name == 'sgd':\n",
        "        optimizer = SGD(learning_rate=learning_rate)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid optimizer name\")\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define the fitness function\n",
        "def fitness(params):\n",
        "    learning_rate, num_filters, kernel_size, pooling_type, optimizer_name = params\n",
        "    model = create_cnn_model(learning_rate, num_filters, kernel_size, pooling_type, optimizer_name)\n",
        "\n",
        "    # Fit the model for one epoch (adjust as needed)\n",
        "    model.fit(train_generator, epochs=1, verbose=0)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    loss, accuracy = model.evaluate(validation_generator, verbose=0)\n",
        "    return accuracy  # Only return the accuracy metric\n",
        "\n",
        "# Define the Particle Swarm Optimization (PSO) function\n",
        "def particle_swarm_optimization():\n",
        "    # Define hyperparameter ranges (adjust these as needed)\n",
        "    learning_rate_range = (0.001, 0.1)\n",
        "    num_filters_range = (16, 64)\n",
        "    kernel_size_range = (3, 5)\n",
        "    pooling_type_range = ('max', 'avg')\n",
        "    optimizer_range = ('adam', 'rmsprop', 'sgd')\n",
        "\n",
        "    # Initialize particles\n",
        "    num_particles = 10  # Reduced number for testing\n",
        "    particles = []\n",
        "    velocities = []\n",
        "    for _ in range(num_particles):\n",
        "        particle = [\n",
        "            random.uniform(*learning_rate_range),\n",
        "            random.randint(*num_filters_range),\n",
        "            random.randint(*kernel_size_range),\n",
        "            random.choice(pooling_type_range),\n",
        "            random.choice(optimizer_range)\n",
        "        ]\n",
        "        velocity = [\n",
        "            random.uniform(-1, 1),\n",
        "            random.uniform(-1, 1),\n",
        "            random.uniform(-1, 1),\n",
        "            random.uniform(-1, 1),\n",
        "            random.uniform(-1, 1)\n",
        "        ]\n",
        "        particles.append(particle)\n",
        "        velocities.append(velocity)\n",
        "\n",
        "    # Personal best and global best\n",
        "    pbest = particles.copy()\n",
        "    pbest_fitness = [fitness(p) for p in particles]\n",
        "    gbest = particles[np.argmax(pbest_fitness)]\n",
        "    gbest_fitness = max(pbest_fitness)\n",
        "\n",
        "    # Particle Swarm Optimization\n",
        "    for iteration in range(10):  # Reduced iterations for testing\n",
        "        for i in range(num_particles):\n",
        "            # Update velocity\n",
        "            w = 0.9\n",
        "            c1 = 2\n",
        "            c2 = 2\n",
        "            r1 = random.random()\n",
        "            r2 = random.random()\n",
        "            velocities[i] = [\n",
        "                w * velocities[i][0] + c1 * r1 * (pbest[i][0] - particles[i][0]) + c2 * r2 * (gbest[0] - particles[i][0]),\n",
        "                w * velocities[i][1] + c1 * r1 * (pbest[i][1] - particles[i][1]) + c2 * r2 * (gbest[1] - particles[i][1]),\n",
        "                w * velocities[i][2] + c1 * r1 * (pbest[i][2] - particles[i][2]) + c2 * r2 * (gbest[2] - particles[i][2]),\n",
        "                velocities[i][3],  # No change for categorical params\n",
        "                velocities[i][4]   # No change for categorical params\n",
        "            ]\n",
        "\n",
        "            # Update particle position\n",
        "            particles[i] = [\n",
        "                particles[i][0] + velocities[i][0],\n",
        "                int(particles[i][1] + velocities[i][1]),  # Ensure integer values\n",
        "                int(particles[i][2] + velocities[i][2]),  # Ensure integer values\n",
        "                particles[i][3],\n",
        "                particles[i][4]\n",
        "            ]\n",
        "\n",
        "            # Ensure the hyperparameters are within valid ranges\n",
        "            particles[i][0] = max(min(particles[i][0], learning_rate_range[1]), learning_rate_range[0])\n",
        "            particles[i][1] = max(min(particles[i][1], num_filters_range[1]), num_filters_range[0])\n",
        "            particles[i][2] = max(min(particles[i][2], kernel_size_range[1]), kernel_size_range[0])\n",
        "\n",
        "            # Evaluate particle fitness\n",
        "            particle_fitness = fitness(particles[i])\n",
        "\n",
        "            # Update personal best\n",
        "            if particle_fitness > pbest_fitness[i]:\n",
        "                pbest[i] = particles[i]\n",
        "                pbest_fitness[i] = particle_fitness\n",
        "\n",
        "            # Update global best\n",
        "            if particle_fitness > gbest_fitness:\n",
        "                gbest = particles[i]\n",
        "                gbest_fitness = particle_fitness\n",
        "\n",
        "        print(f'Iteration {iteration}, Best accuracy: {gbest_fitness:.4f}')\n",
        "        print(f'Accuracy {gbest_fitness:.4f} of parameters {particles}\\n')\n",
        "\n",
        "        if gbest_fitness > 0.99:  # Arbitrary stopping condition for testing\n",
        "            break\n",
        "\n",
        "    print(f'Best solution: {gbest} with accuracy: {gbest_fitness:.4f}')\n",
        "\n",
        "# Run the Particle Swarm Optimization\n",
        "particle_swarm_optimization()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFBxYeniocLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6b8434a-84de-462d-db98-65ba8787df28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1322 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n",
            "Iteration 0, Best accuracy: 0.6333\n",
            "Iteration 1, Best accuracy: 0.6333\n",
            "Iteration 2, Best accuracy: 0.7000\n",
            "Iteration 3, Best accuracy: 0.7000\n",
            "Iteration 4, Best accuracy: 0.7000\n",
            "Iteration 5, Best accuracy: 0.7000\n",
            "Iteration 6, Best accuracy: 0.7000\n",
            "Iteration 7, Best accuracy: 0.7000\n",
            "Iteration 8, Best accuracy: 0.7000\n",
            "Iteration 9, Best accuracy: 0.7000\n",
            "Best solution: [0.001, 16, 3, 'max', 'adam'] with accuracy: 0.7000\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "\n",
        "# Load your image dataset (replace with your actual data loading code)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_dataset = '/content/drive/MyDrive/datasets/plant_disease/Train/Train'\n",
        "val_dataset = '/content/drive/MyDrive/datasets/plant_disease/Validation/Validation'\n",
        "test_dataset = '/content/drive/MyDrive/datasets/plant_disease/Test/Test'\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dataset,\n",
        "    target_size=(28, 28),  # Adjust image size as needed\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    val_dataset,\n",
        "    target_size=(28, 28),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "def create_cnn_model(learning_rate, num_filters, kernel_size, pooling_type, optimizer_name):\n",
        "    model = Sequential()\n",
        "    kernel_size = int(kernel_size)\n",
        "    num_filters = int(num_filters)\n",
        "\n",
        "    model.add(Conv2D(num_filters, (kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 3)))\n",
        "\n",
        "    if pooling_type == 'max':\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "    elif pooling_type == 'avg':\n",
        "        model.add(AveragePooling2D((2, 2)))\n",
        "    else:\n",
        "        raise ValueError(\"Invalid pooling type\")\n",
        "\n",
        "    model.add(Conv2D(num_filters * 2, (kernel_size, kernel_size), activation='relu'))\n",
        "    model.add(MaxPooling2D((3, 3)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))  # Output layer for 3 classes\n",
        "\n",
        "    if optimizer_name == 'adam':\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer_name == 'rmsprop':\n",
        "        optimizer = RMSprop(learning_rate=learning_rate)\n",
        "    elif optimizer_name == 'sgd':\n",
        "        optimizer = SGD(learning_rate=learning_rate)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid optimizer name\")\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def fitness(params):\n",
        "    learning_rate, num_filters, kernel_size, pooling_type, optimizer_name = params\n",
        "    model = create_cnn_model(learning_rate, num_filters, kernel_size, pooling_type, optimizer_name)\n",
        "\n",
        "    # Fit the model for one epoch (adjust as needed)\n",
        "    model.fit(train_generator, epochs=1, verbose=0)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    loss, accuracy = model.evaluate(validation_generator, verbose=0)\n",
        "    return accuracy  # Only return the accuracy metric\n",
        "\n",
        "def particle_swarm_optimization():\n",
        "    # Define hyperparameter ranges (adjust these as needed)\n",
        "    learning_rate_range = (0.001, 0.1)\n",
        "    num_filters_range = (16, 64)\n",
        "    kernel_size_range = (3, 5)\n",
        "    pooling_type_range = ('max', 'avg')\n",
        "    optimizer_range = ('adam', 'rmsprop', 'sgd')\n",
        "\n",
        "    # Initialize particles\n",
        "    num_particles = 10  # Reduced number for testing\n",
        "    particles = []\n",
        "    velocities = []\n",
        "    for _ in range(num_particles):\n",
        "        particle = [\n",
        "            random.uniform(*learning_rate_range),\n",
        "            random.randint(*num_filters_range),\n",
        "            random.randint(*kernel_size_range),\n",
        "            random.choice(pooling_type_range),\n",
        "            random.choice(optimizer_range)\n",
        "        ]\n",
        "        velocity = [\n",
        "            random.uniform(-1, 1),\n",
        "            random.uniform(-1, 1),\n",
        "            random.uniform(-1, 1),\n",
        "            random.uniform(-1, 1),\n",
        "            random.uniform(-1, 1)\n",
        "        ]\n",
        "        particles.append(particle)\n",
        "        velocities.append(velocity)\n",
        "\n",
        "    # Personal best and global best\n",
        "    pbest = particles.copy()\n",
        "    pbest_fitness = [fitness(p) for p in particles]\n",
        "    gbest = particles[np.argmax(pbest_fitness)]\n",
        "    gbest_fitness = max(pbest_fitness)\n",
        "\n",
        "    # Particle Swarm Optimization\n",
        "    for iteration in range(10):  # Reduced iterations for testing\n",
        "        for i in range(num_particles):\n",
        "            # Update velocity\n",
        "            w = 0.9\n",
        "            c1 = 2\n",
        "            c2 = 2\n",
        "            r1 = random.random()\n",
        "            r2 = random.random()\n",
        "            velocities[i] = [\n",
        "                w * velocities[i][0] + c1 * r1 * (pbest[i][0] - particles[i][0]) + c2 * r2 * (gbest[0] - particles[i][0]),\n",
        "                w * velocities[i][1] + c1 * r1 * (pbest[i][1] - particles[i][1]) + c2 * r2 * (gbest[1] - particles[i][1]),\n",
        "                w * velocities[i][2] + c1 * r1 * (pbest[i][2] - particles[i][2]) + c2 * r2 * (gbest[2] - particles[i][2]),\n",
        "                velocities[i][3],  # No change for categorical params\n",
        "                velocities[i][4]   # No change for categorical params\n",
        "            ]\n",
        "\n",
        "            # Update particle position\n",
        "            particles[i] = [\n",
        "                particles[i][0] + velocities[i][0],\n",
        "                int(particles[i][1] + velocities[i][1]),  # Ensure integer values\n",
        "                int(particles[i][2] + velocities[i][2]),  # Ensure integer values\n",
        "                particles[i][3],\n",
        "                particles[i][4]\n",
        "            ]\n",
        "\n",
        "            # Ensure the hyperparameters are within valid ranges\n",
        "            particles[i][0] = max(min(particles[i][0], learning_rate_range[1]), learning_rate_range[0])\n",
        "            particles[i][1] = max(min(particles[i][1], num_filters_range[1]), num_filters_range[0])\n",
        "            particles[i][2] = max(min(particles[i][2], kernel_size_range[1]), kernel_size_range[0])\n",
        "\n",
        "            # Evaluate particle fitness\n",
        "            particle_fitness = fitness(particles[i])\n",
        "\n",
        "            # Update personal best\n",
        "            if particle_fitness > pbest_fitness[i]:\n",
        "                pbest[i] = particles[i]\n",
        "                pbest_fitness[i] = particle_fitness\n",
        "\n",
        "            # Update global best\n",
        "            if particle_fitness > gbest_fitness:\n",
        "                gbest = particles[i]\n",
        "                gbest_fitness = particle_fitness\n",
        "\n",
        "        print(f'Iteration {iteration}, Best accuracy: {gbest_fitness:.4f}')\n",
        "\n",
        "        if gbest_fitness > 0.99:  # Arbitrary stopping condition for testing\n",
        "            break\n",
        "\n",
        "    print(f'Best solution: {gbest} with accuracy: {gbest_fitness:.4f}')\n",
        "\n",
        "# Run the Particle Swarm Optimization\n",
        "particle_swarm_optimization()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvO2oyoH6n63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1dd855b-867c-4b8f-c9c5-1215a0a8eee5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1322 images belonging to 3 classes.\n",
            "Found 60 images belonging to 3 classes.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x783d8fb7c0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x783d8fafc160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Best accuracy: 0.5667\n",
            "Iteration 1, Best accuracy: 0.5667\n",
            "Iteration 2, Best accuracy: 0.5667\n",
            "Iteration 3, Best accuracy: 0.5667\n",
            "Iteration 4, Best accuracy: 0.6000\n",
            "Iteration 5, Best accuracy: 0.6000\n",
            "Iteration 6, Best accuracy: 0.6000\n",
            "Iteration 7, Best accuracy: 0.6000\n",
            "Iteration 8, Best accuracy: 0.6000\n",
            "Iteration 9, Best accuracy: 0.6000\n",
            "Best solution: [0.001, 16, 3, 'avg', 'rmsprop'] with accuracy: 0.6000\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "\n",
        "# Load your image dataset (replace with your actual data loading code)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_dataset = '/content/drive/MyDrive/datasets/plant_disease/Train/Train'\n",
        "val_dataset = '/content/drive/MyDrive/datasets/plant_disease/Validation/Validation'\n",
        "test_dataset = '/content/drive/MyDrive/datasets/plant_disease/Test/Test'\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dataset,\n",
        "    target_size=(28, 28),  # Adjust image size as needed\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    val_dataset,\n",
        "    target_size=(28, 28),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "def create_cnn_model(learning_rate, num_filters, kernel_size, pooling_type, optimizer_name):\n",
        "    model = Sequential()\n",
        "    kernel_size = int(kernel_size)\n",
        "    num_filters = int(num_filters)\n",
        "\n",
        "    model.add(Conv2D(num_filters, (kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 3)))\n",
        "\n",
        "    if pooling_type == 'max':\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "    elif pooling_type == 'avg':\n",
        "        model.add(AveragePooling2D((2, 2)))\n",
        "    else:\n",
        "        raise ValueError(\"Invalid pooling type\")\n",
        "\n",
        "    model.add(Conv2D(num_filters * 2, (kernel_size, kernel_size), activation='relu'))\n",
        "    model.add(MaxPooling2D((3, 3)))\n",
        "    model.add(Conv2D(num_filters * 2, (kernel_size, kernel_size), padding='same', activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))  # Output layer for 3 classes\n",
        "\n",
        "    if optimizer_name == 'adam':\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer_name == 'rmsprop':\n",
        "        optimizer = RMSprop(learning_rate=learning_rate)\n",
        "    elif optimizer_name == 'sgd':\n",
        "        optimizer = SGD(learning_rate=learning_rate)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid optimizer name\")\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def fitness(params):\n",
        "    learning_rate, num_filters, kernel_size, pooling_type, optimizer_name = params\n",
        "    model = create_cnn_model(learning_rate, num_filters, kernel_size, pooling_type, optimizer_name)\n",
        "\n",
        "    # Fit the model for one epoch (adjust as needed)\n",
        "    model.fit(train_generator, epochs=1, verbose=0)\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    loss, accuracy = model.evaluate(validation_generator, verbose=0)\n",
        "    return accuracy  # Only return the accuracy metric\n",
        "\n",
        "def particle_swarm_optimization():\n",
        "    # Define hyperparameter ranges (adjust these as needed)\n",
        "    learning_rate_range = (0.001, 0.1)\n",
        "    num_filters_range = (16, 64)\n",
        "    kernel_size_range = (3, 5)\n",
        "    pooling_type_range = ('max', 'avg')\n",
        "    optimizer_range = ('adam', 'rmsprop', 'sgd')\n",
        "\n",
        "    # Initialize particles\n",
        "    num_particles = 10  # Reduced number for testing\n",
        "    particles = []\n",
        "    velocities = []\n",
        "    for _ in range(num_particles):\n",
        "        particle = [\n",
        "            random.uniform(*learning_rate_range),\n",
        "            random.randint(*num_filters_range),\n",
        "            random.randint(*kernel_size_range),\n",
        "            random.choice(pooling_type_range),\n",
        "            random.choice(optimizer_range)\n",
        "        ]\n",
        "        velocity = [\n",
        "            random.uniform(-1, 1),\n",
        "            random.uniform(-1, 1),\n",
        "            random.uniform(-1, 1),\n",
        "            random.uniform(-1, 1),\n",
        "            random.uniform(-1, 1)\n",
        "        ]\n",
        "        particles.append(particle)\n",
        "        velocities.append(velocity)\n",
        "\n",
        "    # Personal best and global best\n",
        "    pbest = particles.copy()\n",
        "    pbest_fitness = [fitness(p) for p in particles]\n",
        "    gbest = particles[np.argmax(pbest_fitness)]\n",
        "    gbest_fitness = max(pbest_fitness)\n",
        "\n",
        "    # Particle Swarm Optimization\n",
        "    for iteration in range(10):  # Reduced iterations for testing\n",
        "        for i in range(num_particles):\n",
        "            # Update velocity\n",
        "            w = 0.9\n",
        "            c1 = 2\n",
        "            c2 = 2\n",
        "            r1 = random.random()\n",
        "            r2 = random.random()\n",
        "            velocities[i] = [\n",
        "                w * velocities[i][0] + c1 * r1 * (pbest[i][0] - particles[i][0]) + c2 * r2 * (gbest[0] - particles[i][0]),\n",
        "                w * velocities[i][1] + c1 * r1 * (pbest[i][1] - particles[i][1]) + c2 * r2 * (gbest[1] - particles[i][1]),\n",
        "                w * velocities[i][2] + c1 * r1 * (pbest[i][2] - particles[i][2]) + c2 * r2 * (gbest[2] - particles[i][2]),\n",
        "                velocities[i][3],\n",
        "                velocities[i][4]\n",
        "            ]\n",
        "              '''\n",
        "              velocities[i] is the velocity of particle i\n",
        "              w is the inertia weight\n",
        "              c1 and c2 are the cognitive and social acceleration coefficients\n",
        "              r1 and r2 are random numbers between 0 and 1\n",
        "              pbest[i] is the personal best of particle i\n",
        "              gbest is the global best\n",
        "              particles[i] is the position of particle i\n",
        "\n",
        "              '''\n",
        "            # Update particle position\n",
        "            particles[i] = [\n",
        "                particles[i][0] + velocities[i][0],\n",
        "                int(particles[i][1] + velocities[i][1]),\n",
        "                int(particles[i][2] + velocities[i][2]),\n",
        "                particles[i][3],\n",
        "                particles[i][4]\n",
        "            ]\n",
        "\n",
        "            # Ensure the hyperparameters are within valid ranges\n",
        "            particles[i][0] = max(min(particles[i][0], learning_rate_range[1]), learning_rate_range[0])\n",
        "            particles[i][1] = max(min(particles[i][1], num_filters_range[1]), num_filters_range[0])\n",
        "            particles[i][2] = max(min(particles[i][2], kernel_size_range[1]), kernel_size_range[0])\n",
        "\n",
        "            # Evaluate particle fitness\n",
        "            particle_fitness = fitness(particles[i])\n",
        "\n",
        "            # Update personal best\n",
        "            if particle_fitness > pbest_fitness[i]:\n",
        "                pbest[i] = particles[i]\n",
        "                pbest_fitness[i] = particle_fitness\n",
        "\n",
        "            # Update global best\n",
        "            if particle_fitness > gbest_fitness:\n",
        "                gbest = particles[i]\n",
        "                gbest_fitness = particle_fitness\n",
        "\n",
        "        print(f'Iteration {iteration}, Best accuracy: {gbest_fitness:.4f}')\n",
        "\n",
        "        if gbest_fitness > 0.99:  # Arbitrary stopping condition for testing\n",
        "            break\n",
        "\n",
        "    print(f'Best solution: {gbest} with accuracy: {gbest_fitness:.4f}')\n",
        "\n",
        "# Run the Particle Swarm Optimization\n",
        "particle_swarm_optimization()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dZyvVrrIbyE_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_D0moE0N_6seC7VxxaqlPUKeoabXs4Lu",
      "authorship_tag": "ABX9TyPVvQruiNncgwe2yEZNfmGA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}